{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CryptoAutogluon",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rQhGbgqP7mU0",
        "outputId": "2437f5e8-d1e1-45e8-bf27-affcb3e7527c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.3.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n",
            "Installing collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed setuptools-62.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon\n",
            "  Downloading autogluon-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Collecting autogluon.features==0.4.1\n",
            "  Downloading autogluon.features-0.4.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting autogluon.tabular[all]==0.4.1\n",
            "  Downloading autogluon.tabular-0.4.1-py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 10.2 MB/s \n",
            "\u001b[?25hCollecting autogluon.vision==0.4.1\n",
            "  Downloading autogluon.vision-0.4.1-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting autogluon.core[all]==0.4.1\n",
            "  Downloading autogluon.core-0.4.1-py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 46.7 MB/s \n",
            "\u001b[?25hCollecting autogluon.text==0.4.1\n",
            "  Downloading autogluon.text-0.4.1-py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<1.4,>=1.2.5 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon) (1.3.5)\n",
            "Requirement already satisfied: numpy<1.23,>=1.21 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn<1.1,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon) (1.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon) (2.23.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon) (3.2.2)\n",
            "Collecting autogluon.common==0.4.1\n",
            "  Downloading autogluon.common-0.4.1-py3-none-any.whl (37 kB)\n",
            "Collecting scipy<1.8.0,>=1.5.4\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting dask<=2021.11.2,>=2021.09.1\n",
            "  Downloading dask-2021.11.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 45.1 MB/s \n",
            "\u001b[?25hCollecting distributed<=2021.11.2,>=2021.09.1\n",
            "  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n",
            "\u001b[K     |████████████████████████████████| 802 kB 47.4 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.23.5-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.4.1->autogluon) (4.64.0)\n",
            "Collecting ray<1.11,>=1.10\n",
            "  Downloading ray-1.10.0-cp37-cp37m-manylinux2014_x86_64.whl (59.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.6 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting psutil<5.9,>=5.7.3\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.4.1->autogluon) (2.6.3)\n",
            "Collecting xgboost<1.5,>=1.4\n",
            "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 166.7 MB 18 kB/s \n",
            "\u001b[?25hCollecting torch<1.11,>=1.0\n",
            "  Downloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.3 MB/s eta 0:00:37tcmalloc: large alloc 1147494400 bytes == 0x3244000 @  0x7f87808c0615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 2.0 kB/s \n",
            "\u001b[?25hCollecting fastai<2.6,>=2.3.1\n",
            "  Downloading fastai-2.5.6-py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting lightgbm<3.4,>=3.3\n",
            "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 32.6 MB/s \n",
            "\u001b[?25hCollecting catboost<1.1,>=1.0\n",
            "  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting nptyping<1.5.0,>=1.4.4\n",
            "  Downloading nptyping-1.4.4-py3-none-any.whl (31 kB)\n",
            "Collecting omegaconf<2.2.0,>=2.1.1\n",
            "  Downloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting fairscale<0.5.0,>=0.4.5\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 47.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece<0.2.0,>=0.1.95\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 38.9 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning<1.7.0,>=1.5.10\n",
            "  Downloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\n",
            "\u001b[K     |████████████████████████████████| 584 kB 54.0 MB/s \n",
            "\u001b[?25hCollecting timm<0.6.0\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 50.1 MB/s \n",
            "\u001b[?25hCollecting setuptools<=59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 43.6 MB/s \n",
            "\u001b[?25hCollecting autogluon-contrib-nlp==0.0.1b20220208\n",
            "  Downloading autogluon_contrib_nlp-0.0.1b20220208-py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 49.1 MB/s \n",
            "\u001b[?25hCollecting torchmetrics<0.8.0,>=0.7.2\n",
            "  Downloading torchmetrics-0.7.3-py3-none-any.whl (398 kB)\n",
            "\u001b[K     |████████████████████████████████| 398 kB 13.5 MB/s \n",
            "\u001b[?25hCollecting Pillow<9.1.0,>=9.0.1\n",
            "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 28.2 MB/s \n",
            "\u001b[?25hCollecting smart-open<5.3.0,>=5.2.1\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting transformers<4.17.0,>=4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 39.0 MB/s \n",
            "\u001b[?25hCollecting scikit-image<0.20.0,>=0.19.1\n",
            "  Downloading scikit_image-0.19.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 39.7 MB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "  Downloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece<0.2.0,>=0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 44.3 MB/s \n",
            "\u001b[?25hCollecting contextvars\n",
            "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 10.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon) (6.0.1)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting sacremoses>=0.0.38\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 40.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon) (3.17.3)\n",
            "Collecting tokenizers>=0.9.4\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 30.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon) (2019.12.20)\n",
            "Collecting gluoncv<0.10.6,>=0.10.5\n",
            "  Downloading gluoncv-0.10.5.post0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 37.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost<1.1,>=1.0->autogluon.tabular[all]==0.4.1->autogluon) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost<1.1,>=1.0->autogluon.tabular[all]==0.4.1->autogluon) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost<1.1,>=1.0->autogluon.tabular[all]==0.4.1->autogluon) (5.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (3.13)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (0.11.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (21.3)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (1.3.0)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (2.4.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (5.1.1)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (1.0.3)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (7.1.2)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (1.7.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (2.2.0)\n",
            "Collecting cloudpickle>=1.1.1\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (2.11.3)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (0.12.0+cu113)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (1.0.2)\n",
            "Collecting fastcore<1.5,>=1.3.27\n",
            "  Downloading fastcore-1.4.3-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (21.1.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (2.2.4)\n",
            "Collecting fastdownload<2,>=0.0.5\n",
            "  Downloading fastdownload-0.0.6-py3-none-any.whl (12 kB)\n",
            "Collecting autocfg\n",
            "  Downloading autocfg-0.0.8-py3-none-any.whl (13 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.6,>=0.10.5->autogluon.vision==0.4.1->autogluon) (4.1.2.30)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm<3.4,>=3.3->autogluon.tabular[all]==0.4.1->autogluon) (0.37.1)\n",
            "Collecting typish>=1.7.0\n",
            "  Downloading typish-1.9.3-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.4,>=1.2.5->autogluon.core[all]==0.4.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.4,>=1.2.5->autogluon.core[all]==0.4.1->autogluon) (2022.1)\n",
            "Collecting locket\n",
            "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Collecting pyDeprecate<0.4.0,>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (4.2.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 41.9 MB/s \n",
            "\u001b[?25hCollecting redis>=3.5.0\n",
            "  Downloading redis-4.3.1-py3-none-any.whl (241 kB)\n",
            "\u001b[K     |████████████████████████████████| 241 kB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon) (4.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon) (3.7.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon) (21.4.0)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon) (1.46.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon) (4.11.3)\n",
            "Collecting async-timeout>=4.0.2\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting deprecated>=1.2.3\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon) (1.14.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon) (3.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses>=0.0.38->autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon) (1.1.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.text==0.4.1->autogluon) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.text==0.4.1->autogluon) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.text==0.4.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1,>=1.0.0->autogluon.core[all]==0.4.1->autogluon) (3.1.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (0.9.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (1.0.7)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.6,>=2.3.1->autogluon.tabular[all]==0.4.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.4.1->autogluon) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.4.1->autogluon) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.4.1->autogluon) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.4.1->autogluon) (1.24.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.5.10->autogluon.text==0.4.1->autogluon) (3.2.0)\n",
            "Collecting torchvision>=0.8.2\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 1.4 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.11.3-cp37-cp37m-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 21.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 54.1 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.4 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting botocore<1.27.0,>=1.26.5\n",
            "  Downloading botocore-1.26.5-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 39.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 44.9 MB/s \n",
            "\u001b[?25hCollecting immutables>=0.9\n",
            "  Downloading immutables-0.18-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting pycodestyle<2.9.0,>=2.8.0\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 931 kB/s \n",
            "\u001b[?25hCollecting pyflakes<2.5.0,>=2.4.0\n",
            "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting flake8\n",
            "  Downloading flake8-4.0.0-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s \n",
            "\u001b[?25h  Downloading flake8-3.9.2-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting pycodestyle<2.8.0,>=2.7.0\n",
            "  Downloading pycodestyle-2.7.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 616 kB/s \n",
            "\u001b[?25hCollecting pyflakes<2.4.0,>=2.3.0\n",
            "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.4.1->autogluon) (2.0.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray<1.11,>=1.10->autogluon.core[all]==0.4.1->autogluon) (5.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core[all]==0.4.1->autogluon) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core[all]==0.4.1->autogluon) (1.4.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost<1.1,>=1.0->autogluon.tabular[all]==0.4.1->autogluon) (8.0.1)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.4.1->autogluon) (0.8.9)\n",
            "Building wheels for collected packages: fairscale, antlr4-python3-runtime, sacremoses, contextvars\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307225 sha256=6f539f4a88280044a48d70002553c919881826d1528fb623b3ebaebde4ef8756\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=d6987a5aa534d59e9dfac2642d492a89af687132dc76b2ee4cc36e0270d288d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=79f28bb5d5e9839c0cd56a72c6d5c565c796375c5fd83b3b99ebf9f955854aa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7664 sha256=a48aace137e5a52606c11cb1d37bb81057d611228e986e721ef02f772937f7b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/11/79/e70e668095c0bb1f94718af672ef2d35ee7a023fee56ef54d9\n",
            "Successfully built fairscale antlr4-python3-runtime sacremoses contextvars\n",
            "Installing collected packages: urllib3, jmespath, locket, botocore, setuptools, s3transfer, pyyaml, partd, multidict, fsspec, frozenlist, cloudpickle, yarl, scipy, psutil, dask, boto3, asynctest, async-timeout, aiosignal, torch, pyflakes, pyDeprecate, pycodestyle, portalocker, Pillow, mccabe, immutables, fastcore, distributed, deprecated, colorama, autogluon.common, aiohttp, yacs, typish, torchvision, torchmetrics, tokenizers, sentencepiece, sacremoses, sacrebleu, redis, huggingface-hub, flake8, fastdownload, contextvars, autogluon.features, autogluon.core, autocfg, antlr4-python3-runtime, xgboost, transformers, timm, smart-open, scikit-image, ray, pytorch-lightning, omegaconf, nptyping, lightgbm, gluoncv, fastai, fairscale, catboost, autogluon.tabular, autogluon-contrib-nlp, autogluon.vision, autogluon.text, autogluon\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 62.3.2\n",
            "    Uninstalling setuptools-62.3.2:\n",
            "      Successfully uninstalled setuptools-62.3.2\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-9.0.1 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 asynctest-0.13.0 autocfg-0.0.8 autogluon-0.4.1 autogluon-contrib-nlp-0.0.1b20220208 autogluon.common-0.4.1 autogluon.core-0.4.1 autogluon.features-0.4.1 autogluon.tabular-0.4.1 autogluon.text-0.4.1 autogluon.vision-0.4.1 boto3-1.23.5 botocore-1.26.5 catboost-1.0.6 cloudpickle-2.1.0 colorama-0.4.4 contextvars-2.4 dask-2021.11.2 deprecated-1.2.13 distributed-2021.11.2 fairscale-0.4.6 fastai-2.5.6 fastcore-1.4.3 fastdownload-0.0.6 flake8-3.9.2 frozenlist-1.3.0 fsspec-2022.5.0 gluoncv-0.10.5.post0 huggingface-hub-0.6.0 immutables-0.18 jmespath-1.0.0 lightgbm-3.3.2 locket-1.0.0 mccabe-0.6.1 multidict-6.0.2 nptyping-1.4.4 omegaconf-2.1.2 partd-1.2.0 portalocker-2.4.0 psutil-5.8.0 pyDeprecate-0.3.2 pycodestyle-2.7.0 pyflakes-2.3.1 pytorch-lightning-1.6.3 pyyaml-6.0 ray-1.10.0 redis-4.3.1 s3transfer-0.5.2 sacrebleu-2.1.0 sacremoses-0.0.53 scikit-image-0.19.2 scipy-1.7.3 sentencepiece-0.1.95 setuptools-59.5.0 smart-open-5.2.1 timm-0.5.4 tokenizers-0.12.1 torch-1.10.2 torchmetrics-0.7.3 torchvision-0.11.3 transformers-4.16.2 typish-1.9.3 urllib3-1.25.11 xgboost-1.4.2 yacs-0.1.8 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "contextvars",
                  "pkg_resources",
                  "psutil",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U setuptools wheel\n",
        "!pip install autogluon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EXeQxpVAEuVg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "dcf1027d-c6ef-4e38-906c-46d074b0fef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2a5f497a0384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0;34m\"standardized_allTweets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"standardized_allPosts\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"standardized_allTrends\"]\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbitcoinDataTrainInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbitcoinDataFull\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m548\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mallList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#excluding final month\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mbitcoinDataTrainOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbitcoinDataFull\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m548\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"standardized_price\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbitcoinDataTestInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbitcoinDataFull\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m548\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mallList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#final month\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bitcoinDataFull' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# adapted from https://towardsdatascience.com/pytorch-lstms-for-time-series-data-cd16190929d7\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, hidden_layers=64):\n",
        "        input_size = 7\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_layers = hidden_layers\n",
        "        # lstm1, lstm2, linear are all layers in the network\n",
        "        self.lstm1 = nn.LSTMCell(input_size, self.hidden_layers)\n",
        "        self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
        "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
        "        \n",
        "    def forward(self, y, test_inputs=()):\n",
        "        outputs, n_samples, n_features = [], y.size(0), y.size(2)\n",
        "        h_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        c_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        h_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        c_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        \n",
        "        for time_step in y.split(1, dim=1):\n",
        "            # N, 1\n",
        "            time_step = torch.squeeze(time_step, dim=1)\n",
        "            h_t, c_t = self.lstm1(time_step.float(), (h_t.float(), c_t.float())) # initial hidden and cell states\n",
        "            #h_t2, c_t2 = self.lstm2(h_t.float(), (h_t2.float(), c_t2.float())) # new hidden and cell states\n",
        "            output = self.linear(h_t.float()) # output from the last FC layer\n",
        "            outputs.append(output)\n",
        "            \n",
        "        for tester in test_inputs:\n",
        "            # this only generates future predictions if we pass in future_preds>0\n",
        "            # mirrors the code above, using last output/prediction as input\n",
        "            h_t, c_t = self.lstm1(tester.float(), (h_t.float(), c_t.float()))\n",
        "            #h_t2, c_t2 = self.lstm2(h_t.float(), (h_t2.float(), c_t2.float()))\n",
        "            output = self.linear(h_t.float())\n",
        "            outputs.append(output)\n",
        "        # transform list to tensor    \n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "def training_loop(n_epochs, model, optimiser, loss_fn, \n",
        "                  train_input, train_target, test_input, test_target):\n",
        "    for i in range(n_epochs):\n",
        "        def closure():\n",
        "            optimiser.zero_grad()\n",
        "            out = model(train_input.float())\n",
        "            loss = loss_fn(out, torch.squeeze(train_target, 2))\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimiser.step(closure)\n",
        "        with torch.no_grad():\n",
        "            pred = model(test_input)\n",
        "            # use all pred samples, but only go to 999\n",
        "            loss = loss_fn(pred, torch.squeeze(test_target, 2))\n",
        "            print(\"TEST LOSS: \" + str(loss))\n",
        "            y = pred.detach().numpy()\n",
        "        # draw figures\n",
        "        '''plt.figure(figsize=(12,6))\n",
        "        plt.title(f\"Step {i+1}\")\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.xticks(fontsize=20)\n",
        "        plt.yticks(fontsize=20)\n",
        "        n = train_input.shape[1] # 999\n",
        "        def draw(yi, colour):\n",
        "            plt.plot(np.arange(n), yi[:n], colour, linewidth=2.0)\n",
        "            plt.plot(np.arange(n, n+future), yi[n:], colour+\":\", linewidth=2.0)\n",
        "        draw(y[0], 'r')\n",
        "        draw(y[1], 'b')\n",
        "        draw(y[2], 'g')\n",
        "        plt.savefig(\"predict%d.png\"%i, dpi=200)\n",
        "        plt.close()'''\n",
        "        # print the loss\n",
        "        out = model(train_input)\n",
        "        loss_print = loss_fn(out, torch.squeeze(train_target, 2))\n",
        "        print(\"Step: {}, Loss: {}\".format(i, loss_print))"
      ],
      "metadata": {
        "id": "C3wnW0GrAmn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i8T8qPiE3Ns",
        "outputId": "c38c7475-ed27-451e-c614-b2c29c3af96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdrive/MyDrive/CryptoDataFull.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzWtGlQcS6RG",
        "outputId": "94c9ca66-fb4a-440d-cfeb-c6edc68ed793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/CryptoDataFull.zip\n",
            "   creating: CryptoDataFull/\n",
            "  inflating: CryptoDataFull/allcrypto_postsToEnd2021.csv  \n",
            "  inflating: CryptoDataFull/allcrypto_postsToEnd2020.csv  \n",
            "  inflating: CryptoDataFull/dogeMarSep21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._dogeMarSep21.csv  \n",
            "  inflating: CryptoDataFull/Transformation.py  \n",
            "  inflating: __MACOSX/CryptoDataFull/._Transformation.py  \n",
            "  inflating: CryptoDataFull/ethTrends.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._ethTrends.csv  \n",
            "  inflating: CryptoDataFull/etherAugFeb21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._etherAugFeb21.csv  \n",
            "  inflating: CryptoDataFull/bitcoinTrends.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._bitcoinTrends.csv  \n",
            "  inflating: CryptoDataFull/GLMBitcoin.ipynb  \n",
            "  inflating: CryptoDataFull/VisualizeR^2Dec.ipynb  \n",
            "  inflating: CryptoDataFull/EtherWorldAugFeb21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._EtherWorldAugFeb21.csv  \n",
            "  inflating: CryptoDataFull/allOctFeb22.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._allOctFeb22.csv  \n",
            "  inflating: CryptoDataFull/DogeWorldOctFeb22.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._DogeWorldOctFeb22.csv  \n",
            "  inflating: CryptoDataFull/ETH-USD.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._ETH-USD.csv  \n",
            "  inflating: CryptoDataFull/.DS_Store  \n",
            "  inflating: __MACOSX/CryptoDataFull/._.DS_Store  \n",
            "  inflating: CryptoDataFull/ErrorVis.py  \n",
            "  inflating: CryptoDataFull/dogeOctFeb22.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._dogeOctFeb22.csv  \n",
            "  inflating: CryptoDataFull/SHIB-USD (1).csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._SHIB-USD (1).csv  \n",
            "  inflating: CryptoDataFull/bitcoin-twitter.csv  \n",
            "  inflating: CryptoDataFull/DogeWorldMarSep21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._DogeWorldMarSep21.csv  \n",
            "  inflating: CryptoDataFull/allMarSep21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._allMarSep21.csv  \n",
            "  inflating: CryptoDataFull/etherMarSep21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._etherMarSep21.csv  \n",
            "  inflating: CryptoDataFull/shiba_transactions.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._shiba_transactions.csv  \n",
            "  inflating: CryptoDataFull/Bitcoin_Transactions.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._Bitcoin_Transactions.csv  \n",
            "  inflating: CryptoDataFull/dogeAugFeb21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._dogeAugFeb21.csv  \n",
            "  inflating: CryptoDataFull/InitialAnalysis.ipynb  \n",
            "  inflating: CryptoDataFull/shiba_posts.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._shiba_posts.csv  \n",
            "  inflating: CryptoDataFull/shiba_twitter_full.csv  \n",
            "  inflating: CryptoDataFull/EtherWorldMarSep21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._EtherWorldMarSep21.csv  \n",
            "  inflating: CryptoDataFull/etherOctFeb22.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._etherOctFeb22.csv  \n",
            "  inflating: CryptoDataFull/allcrypto-twitter-verified.csv  \n",
            "  inflating: CryptoDataFull/dogeTrends.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._dogeTrends.csv  \n",
            "   creating: CryptoDataFull/__pycache__/\n",
            "  inflating: CryptoDataFull/allcrypto_postsToMar2022.csv  \n",
            "  inflating: CryptoDataFull/Doge Analysis.ipynb  \n",
            "  inflating: CryptoDataFull/allAugFeb21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._allAugFeb21.csv  \n",
            "  inflating: CryptoDataFull/EtherWorldOctFeb22.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._EtherWorldOctFeb22.csv  \n",
            "  inflating: CryptoDataFull/Ethereum Analysis.ipynb  \n",
            "  inflating: CryptoDataFull/DogeWorldAugFeb21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._DogeWorldAugFeb21.csv  \n",
            "  inflating: CryptoDataFull/doge_transactions.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._doge_transactions.csv  \n",
            "  inflating: CryptoDataFull/allcrypto_postsToMid2021.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._allcrypto_postsToMid2021.csv  \n",
            "  inflating: CryptoDataFull/shibaAugFeb21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._shibaAugFeb21.csv  \n",
            "  inflating: CryptoDataFull/DOGE-USD.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._DOGE-USD.csv  \n",
            "  inflating: CryptoDataFull/bitcoinMarSep21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._bitcoinMarSep21.csv  \n",
            "  inflating: CryptoDataFull/AllQuadModels.ipynb  \n",
            "  inflating: CryptoDataFull/eth_twitter_full.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._eth_twitter_full.csv  \n",
            "  inflating: CryptoDataFull/Shiba Analysis.ipynb  \n",
            "  inflating: CryptoDataFull/shibaTrends.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._shibaTrends.csv  \n",
            "  inflating: CryptoDataFull/shiba_Transactions.numbers  \n",
            "  inflating: __MACOSX/CryptoDataFull/._shiba_Transactions.numbers  \n",
            "  inflating: CryptoDataFull/bitcoinOctFeb22.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._bitcoinOctFeb22.csv  \n",
            "  inflating: CryptoDataFull/eth_transactions.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._eth_transactions.csv  \n",
            "  inflating: CryptoDataFull/BitcoinWorldAugFeb21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._BitcoinWorldAugFeb21.csv  \n",
            "  inflating: CryptoDataFull/BTC-USD (1).csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._BTC-USD (1).csv  \n",
            "  inflating: CryptoDataFull/ShibaWorldAugFeb21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._ShibaWorldAugFeb21.csv  \n",
            "   creating: CryptoDataFull/.ipynb_checkpoints/\n",
            "  inflating: CryptoDataFull/DataCleaning.py  \n",
            "  inflating: __MACOSX/CryptoDataFull/._DataCleaning.py  \n",
            "  inflating: CryptoDataFull/Bitcoin Analysis.ipynb  \n",
            "  inflating: CryptoDataFull/dogecoin_posts.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._dogecoin_posts.csv  \n",
            "  inflating: CryptoDataFull/shibaMarSep21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._shibaMarSep21.csv  \n",
            "  inflating: CryptoDataFull/BitcoinWorldOctFeb22.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._BitcoinWorldOctFeb22.csv  \n",
            "  inflating: CryptoDataFull/bitcoinAugFeb21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._bitcoinAugFeb21.csv  \n",
            "  inflating: CryptoDataFull/ShibaWorldOctFeb22.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._ShibaWorldOctFeb22.csv  \n",
            "  inflating: CryptoDataFull/bitcoin_posts.csv  \n",
            "  inflating: CryptoDataFull/ethereum-reddit.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._ethereum-reddit.csv  \n",
            "  inflating: CryptoDataFull/shibaOctFeb22.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._shibaOctFeb22.csv  \n",
            "  inflating: CryptoDataFull/ShibaWorldMarSep21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._ShibaWorldMarSep21.csv  \n",
            "  inflating: CryptoDataFull/Transformation.ipynb  \n",
            "  inflating: CryptoDataFull/BitcoinWorldMarSep21.csv  \n",
            "  inflating: __MACOSX/CryptoDataFull/._BitcoinWorldMarSep21.csv  \n",
            "  inflating: CryptoDataFull/dogecoin_twitter_full.csv  \n",
            "  inflating: CryptoDataFull/__pycache__/ErrorVis.cpython-310.pyc  \n",
            "  inflating: CryptoDataFull/__pycache__/DataCleaning.cpython-310.pyc  \n",
            "  inflating: CryptoDataFull/__pycache__/Transformation.cpython-310.pyc  \n",
            "  inflating: CryptoDataFull/.ipynb_checkpoints/Shiba Analysis-checkpoint.ipynb  \n",
            "  inflating: CryptoDataFull/.ipynb_checkpoints/GLMBitcoin-checkpoint.ipynb  \n",
            "  inflating: CryptoDataFull/.ipynb_checkpoints/Doge Analysis-checkpoint.ipynb  \n",
            "  inflating: CryptoDataFull/.ipynb_checkpoints/InitialAnalysis-checkpoint.ipynb  \n",
            "  inflating: CryptoDataFull/.ipynb_checkpoints/Bitcoin Analysis-checkpoint.ipynb  \n",
            "  inflating: CryptoDataFull/.ipynb_checkpoints/VisualizeR^2Dec-checkpoint.ipynb  \n",
            "  inflating: CryptoDataFull/.ipynb_checkpoints/AllQuadModels-checkpoint.ipynb  \n",
            "  inflating: CryptoDataFull/.ipynb_checkpoints/Ethereum Analysis-checkpoint.ipynb  \n",
            "  inflating: CryptoDataFull/.ipynb_checkpoints/Transformation-checkpoint.ipynb  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"CryptoDataFull\")"
      ],
      "metadata": {
        "id": "QBMj8aR7TLTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "allList = [\"standardized_txs\", \"standardized_posts\", \"standardized_tweets\", \"standardized_trends\", \n",
        "   \"standardized_allTweets\", \"standardized_allPosts\",\n",
        "    \"standardized_allTrends\", \"standardized_price\"]\n",
        "import DataCleaning\n",
        "bitcoinDataFull = DataCleaning.getBitcoinData()\n",
        "bitcoinDataTrainInput = bitcoinDataFull[0:456][allList] #excluding final month\n",
        "bitcoinDataTrainOutput = bitcoinDataFull[0:456][[\"standardized_price\"]]\n",
        "bitcoinDataTestInput = bitcoinDataFull[456:][allList] #final month\n",
        "bitcoinDataTestOutput = bitcoinDataFull[456:][[\"standardized_price\"]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik_LoxroTu4I",
        "outputId": "93290f77-1354-445c-e7a5-e102eb595b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DtypeWarning: Columns (83,87,88,91) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  \n",
            "/content/CryptoDataFull/DataCleaning.py:225: DtypeWarning: Columns (83,84) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([bitcoinDailyTwts, bitcoinDailyPrice, bitcoinDailyPosts, bitcoinModTrends, bitcoinDailyTxs, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:225: DtypeWarning: Columns (82,84,86,87,88) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([bitcoinDailyTwts, bitcoinDailyPrice, bitcoinDailyPosts, bitcoinModTrends, bitcoinDailyTxs, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:225: DtypeWarning: Columns (11,22,78,85,88,89,90,91,92,93,94) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([bitcoinDailyTwts, bitcoinDailyPrice, bitcoinDailyPosts, bitcoinModTrends, bitcoinDailyTxs, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:225: DtypeWarning: Columns (81,82,83,87,88,89) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([bitcoinDailyTwts, bitcoinDailyPrice, bitcoinDailyPosts, bitcoinModTrends, bitcoinDailyTxs, getAllData()], axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['allPosts'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TabularDataset(bitcoinDataTrainInput)\n",
        "predictor_age2 = TabularPredictor(label=\"standardized_price\", path = \"newmodel\", eval_metric=\"r2\").fit(train_data, time_limit=60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmsFLXBoLEur",
        "outputId": "a24c00a9-4193-418e-fcae-afc4a2d2cab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ... Time limit = 60s\n",
            "AutoGluon will save models to \"newmodel/\"\n",
            "AutoGluon Version:  0.4.1\n",
            "Python Version:     3.7.13\n",
            "Operating System:   Linux\n",
            "Train Data Rows:    456\n",
            "Train Data Columns: 7\n",
            "Label Column: standardized_price\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1.7003375300983339, -1.705674807055476, -0.15899, 1.03505)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11886.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 7 | ['standardized_txs', 'standardized_posts', 'standardized_tweets', 'standardized_trends', 'standardized_allTweets', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 7 | ['standardized_txs', 'standardized_posts', 'standardized_tweets', 'standardized_trends', 'standardized_allTweets', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t7 features in original data used to generate 7 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.11s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 364, Val Rows: 92\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 59.89s of the 59.88s of remaining time.\n",
            "\t0.9212\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 59.74s of the 59.74s of remaining time.\n",
            "\t0.9281\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 59.62s of the 59.62s of remaining time.\n",
            "\t0.9373\t = Validation score   (r2)\n",
            "\t0.67s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 58.92s of the 58.91s of remaining time.\n",
            "\t0.914\t = Validation score   (r2)\n",
            "\t0.41s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 58.48s of the 58.48s of remaining time.\n",
            "\t0.9452\t = Validation score   (r2)\n",
            "\t0.83s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 57.51s of the 57.51s of remaining time.\n",
            "\t0.9546\t = Validation score   (r2)\n",
            "\t2.67s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 54.83s of the 54.82s of remaining time.\n",
            "\t0.9448\t = Validation score   (r2)\n",
            "\t0.73s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 53.96s of the 53.95s of remaining time.\n",
            "\t0.9116\t = Validation score   (r2)\n",
            "\t0.68s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 53.25s of the 53.25s of remaining time.\n",
            "\t0.9361\t = Validation score   (r2)\n",
            "\t0.92s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 52.31s of the 52.31s of remaining time.\n",
            "\t0.9663\t = Validation score   (r2)\n",
            "\t2.89s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 49.4s of the 49.4s of remaining time.\n",
            "\t0.942\t = Validation score   (r2)\n",
            "\t0.69s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.89s of the 48.15s of remaining time.\n",
            "\t0.9673\t = Validation score   (r2)\n",
            "\t0.5s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.39s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"newmodel/\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TabularPredictor.load(\"newmodel/\").evaluate(bitcoinDataTestInput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD5cbfLWlRTu",
        "outputId": "9b1a8a40-d75c-45b2-d530-ab250fbbb7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: r2 on test data: -0.9045412720337327\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"r2\": -0.9045412720337327,\n",
            "    \"root_mean_squared_error\": -0.7261277353154488,\n",
            "    \"mean_squared_error\": -0.5272614879943428,\n",
            "    \"mean_absolute_error\": -0.6376364297682067,\n",
            "    \"pearsonr\": 0.5060199670192783,\n",
            "    \"median_absolute_error\": -0.6754447929891569\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_absolute_error': -0.6376364297682067,\n",
              " 'mean_squared_error': -0.5272614879943428,\n",
              " 'median_absolute_error': -0.6754447929891569,\n",
              " 'pearsonr': 0.5060199670192783,\n",
              " 'r2': -0.9045412720337327,\n",
              " 'root_mean_squared_error': -0.7261277353154488}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allList = [\"standardized_txs\", \"standardized_posts\", \"standardized_tweets\", \"standardized_trends\", \n",
        "   \"standardized_allTweets\", \"standardized_allPosts\",\n",
        "    \"standardized_allTrends\"]\n",
        "bitcoinDataTrainInput = bitcoinDataFull[0:548][allList] #excluding final month\n",
        "bitcoinDataTrainOutput = bitcoinDataFull[0:548][[\"standardized_price\"]]\n",
        "bitcoinDataTestInput = bitcoinDataFull[548:][allList] #final month\n",
        "bitcoinDataTestOutput = bitcoinDataFull[548:][[\"standardized_price\"]]"
      ],
      "metadata": {
        "id": "jF6ahswWVQFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bitcoinDataTrainInput"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "-qvdskm-WE-b",
        "outputId": "e67fda28-d19c-4605-e7ee-fa999960f4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            standardized_txs  standardized_posts  standardized_tweets  \\\n",
              "2020-08-02          0.234097           -1.283629            -1.237362   \n",
              "2020-08-03          1.244680           -1.401168            -1.076478   \n",
              "2020-08-04          1.140702           -1.332028            -1.115481   \n",
              "2020-08-05          0.320985           -1.214488            -1.032601   \n",
              "2020-08-06          1.748144           -1.159176            -1.013100   \n",
              "...                      ...                 ...                  ...   \n",
              "2022-01-27         -0.469461           -0.274174             0.600615   \n",
              "2022-01-28         -0.278198           -0.523081             0.673744   \n",
              "2022-01-29         -1.017590           -0.391714            -0.111175   \n",
              "2022-01-30         -1.543211           -0.626792            -0.189179   \n",
              "2022-01-31         -0.304606           -0.419370             0.668869   \n",
              "\n",
              "            standardized_trends  standardized_allTweets  \\\n",
              "2020-08-02            -0.736154               -1.075424   \n",
              "2020-08-03            -0.975663               -0.867565   \n",
              "2020-08-04            -1.095418               -0.907538   \n",
              "2020-08-05            -1.035540               -0.923527   \n",
              "2020-08-06            -0.975663               -0.955505   \n",
              "...                         ...                     ...   \n",
              "2022-01-27             0.306034                1.458858   \n",
              "2022-01-28             0.187088                1.314956   \n",
              "2022-01-29            -0.090451                0.315633   \n",
              "2022-01-30            -0.249045                0.051812   \n",
              "2022-01-31            -0.130099                1.179048   \n",
              "\n",
              "            standardized_allPosts  standardized_allTrends  \n",
              "2020-08-02              -1.276847               -1.094051  \n",
              "2020-08-03              -1.275718               -1.117793  \n",
              "2020-08-04              -1.266688               -1.165276  \n",
              "2020-08-05              -1.282491               -1.165276  \n",
              "2020-08-06              -1.254271               -1.141535  \n",
              "...                           ...                     ...  \n",
              "2022-01-27              -0.003594                0.389799  \n",
              "2022-01-28              -0.092767                0.348252  \n",
              "2022-01-29              -0.172910                0.348252  \n",
              "2022-01-30              -0.207901                0.265156  \n",
              "2022-01-31              -0.026169                0.265156  \n",
              "\n",
              "[548 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45d04225-c791-43df-a469-efdc3d9f0cf1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>standardized_txs</th>\n",
              "      <th>standardized_posts</th>\n",
              "      <th>standardized_tweets</th>\n",
              "      <th>standardized_trends</th>\n",
              "      <th>standardized_allTweets</th>\n",
              "      <th>standardized_allPosts</th>\n",
              "      <th>standardized_allTrends</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-08-02</th>\n",
              "      <td>0.234097</td>\n",
              "      <td>-1.283629</td>\n",
              "      <td>-1.237362</td>\n",
              "      <td>-0.736154</td>\n",
              "      <td>-1.075424</td>\n",
              "      <td>-1.276847</td>\n",
              "      <td>-1.094051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-08-03</th>\n",
              "      <td>1.244680</td>\n",
              "      <td>-1.401168</td>\n",
              "      <td>-1.076478</td>\n",
              "      <td>-0.975663</td>\n",
              "      <td>-0.867565</td>\n",
              "      <td>-1.275718</td>\n",
              "      <td>-1.117793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-08-04</th>\n",
              "      <td>1.140702</td>\n",
              "      <td>-1.332028</td>\n",
              "      <td>-1.115481</td>\n",
              "      <td>-1.095418</td>\n",
              "      <td>-0.907538</td>\n",
              "      <td>-1.266688</td>\n",
              "      <td>-1.165276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-08-05</th>\n",
              "      <td>0.320985</td>\n",
              "      <td>-1.214488</td>\n",
              "      <td>-1.032601</td>\n",
              "      <td>-1.035540</td>\n",
              "      <td>-0.923527</td>\n",
              "      <td>-1.282491</td>\n",
              "      <td>-1.165276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-08-06</th>\n",
              "      <td>1.748144</td>\n",
              "      <td>-1.159176</td>\n",
              "      <td>-1.013100</td>\n",
              "      <td>-0.975663</td>\n",
              "      <td>-0.955505</td>\n",
              "      <td>-1.254271</td>\n",
              "      <td>-1.141535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-01-27</th>\n",
              "      <td>-0.469461</td>\n",
              "      <td>-0.274174</td>\n",
              "      <td>0.600615</td>\n",
              "      <td>0.306034</td>\n",
              "      <td>1.458858</td>\n",
              "      <td>-0.003594</td>\n",
              "      <td>0.389799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-01-28</th>\n",
              "      <td>-0.278198</td>\n",
              "      <td>-0.523081</td>\n",
              "      <td>0.673744</td>\n",
              "      <td>0.187088</td>\n",
              "      <td>1.314956</td>\n",
              "      <td>-0.092767</td>\n",
              "      <td>0.348252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-01-29</th>\n",
              "      <td>-1.017590</td>\n",
              "      <td>-0.391714</td>\n",
              "      <td>-0.111175</td>\n",
              "      <td>-0.090451</td>\n",
              "      <td>0.315633</td>\n",
              "      <td>-0.172910</td>\n",
              "      <td>0.348252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-01-30</th>\n",
              "      <td>-1.543211</td>\n",
              "      <td>-0.626792</td>\n",
              "      <td>-0.189179</td>\n",
              "      <td>-0.249045</td>\n",
              "      <td>0.051812</td>\n",
              "      <td>-0.207901</td>\n",
              "      <td>0.265156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-01-31</th>\n",
              "      <td>-0.304606</td>\n",
              "      <td>-0.419370</td>\n",
              "      <td>0.668869</td>\n",
              "      <td>-0.130099</td>\n",
              "      <td>1.179048</td>\n",
              "      <td>-0.026169</td>\n",
              "      <td>0.265156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>548 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45d04225-c791-43df-a469-efdc3d9f0cf1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-45d04225-c791-43df-a469-efdc3d9f0cf1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-45d04225-c791-43df-a469-efdc3d9f0cf1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "np.random.seed(123)\n",
        "new_model = LSTM()\n",
        "criterion = nn.MSELoss()\n",
        "optimiser = torch.optim.Adam(new_model.parameters(), lr=0.01)\n",
        "training_loop(n_epochs = 300,\n",
        "              model = new_model,\n",
        "              optimiser = optimiser,\n",
        "              loss_fn = criterion,\n",
        "              train_input = torch.tensor(bitcoinDataTrainInput.values.astype(np.float32)).unsqueeze(0),\n",
        "              train_target = torch.tensor(bitcoinDataTrainOutput.values.astype(np.float32)).unsqueeze(0),\n",
        "              test_input = torch.tensor(bitcoinDataTestInput.values.astype(np.float32)).unsqueeze(0),\n",
        "              test_target = torch.tensor(bitcoinDataTestOutput.values.astype(np.float32)).unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "PSEWuBXZZMmF",
        "outputId": "e2d0f40a-70d4-4451-91c0-0b6909982421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f7e5eea487f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ex6esxtL2xSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shibaDataFull = DataCleaning.getShibaData()\n",
        "shibaDataTrainInput = shibaDataFull[0:456][allList] #excluding final month\n",
        "shibaDataTrainOutput = shibaDataFull[0:456][[\"standardized_price\"]]\n",
        "shibaDataTestInput = shibaDataFull[456:][allList] #final month\n",
        "shibaDataTestOutput = shibaDataFull[456:][[\"standardized_price\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAVPuiPY9jOy",
        "outputId": "67a73f0a-3c2f-4413-bcd5-2590487fd233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/CryptoDataFull/DataCleaning.py:102: DtypeWarning: Columns (83,84) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:102: DtypeWarning: Columns (82,84,86,87,88) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:102: DtypeWarning: Columns (11,22,78,85,88,89,90,91,92,93,94) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:102: DtypeWarning: Columns (81,82,83,87,88,89) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['allPosts'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TabularDataset(shibaDataTrainInput)\n",
        "predictor_age3 = TabularPredictor(label=\"standardized_price\", path = \"newmodel2\", eval_metric=\"r2\").fit(train_data, time_limit=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho98_7nJcWYf",
        "outputId": "af383f4b-e6d5-409a-e994-d87f0178062d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ... Time limit = 60s\n",
            "AutoGluon will save models to \"newmodel2/\"\n",
            "AutoGluon Version:  0.4.1\n",
            "Python Version:     3.7.13\n",
            "Operating System:   Linux\n",
            "Train Data Rows:    456\n",
            "Train Data Columns: 7\n",
            "Label Column: standardized_price\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
            "\tLabel info (max, min, mean, stddev): (4.328056225426675, -0.710263066905105, -0.3915, 0.6364)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11550.91 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 7 | ['standardized_txs', 'standardized_posts', 'standardized_tweets', 'standardized_trends', 'standardized_allTweets', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 7 | ['standardized_txs', 'standardized_posts', 'standardized_tweets', 'standardized_trends', 'standardized_allTweets', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t7 features in original data used to generate 7 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.11s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 364, Val Rows: 92\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 59.89s of the 59.88s of remaining time.\n",
            "\t0.934\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 59.77s of the 59.76s of remaining time.\n",
            "\t0.9625\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 59.65s of the 59.64s of remaining time.\n",
            "\t0.7994\t = Validation score   (r2)\n",
            "\t0.61s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 59.0s of the 58.99s of remaining time.\n",
            "\t0.8213\t = Validation score   (r2)\n",
            "\t0.62s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 58.35s of the 58.34s of remaining time.\n",
            "\t0.9478\t = Validation score   (r2)\n",
            "\t0.73s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 57.49s of the 57.48s of remaining time.\n",
            "\t0.9475\t = Validation score   (r2)\n",
            "\t37.05s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 20.41s of the 20.4s of remaining time.\n",
            "\t0.9692\t = Validation score   (r2)\n",
            "\t0.63s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 19.65s of the 19.64s of remaining time.\n",
            "\t0.9493\t = Validation score   (r2)\n",
            "\t0.6s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 19.01s of the 19.0s of remaining time.\n",
            "\t0.9612\t = Validation score   (r2)\n",
            "\t0.49s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 18.5s of the 18.49s of remaining time.\n",
            "\t0.9685\t = Validation score   (r2)\n",
            "\t3.26s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 15.21s of the 15.2s of remaining time.\n",
            "\t0.934\t = Validation score   (r2)\n",
            "\t0.69s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.89s of the 13.96s of remaining time.\n",
            "\t0.9871\t = Validation score   (r2)\n",
            "\t0.46s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 46.54s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"newmodel2/\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TabularPredictor.load(\"newmodel2/\").evaluate(shibaDataTestInput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6pxQyBcde2b",
        "outputId": "9a678afd-16bc-4d42-effc-09575f4938c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: r2 on test data: -0.5873478822793081\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"r2\": -0.5873478822793081,\n",
            "    \"root_mean_squared_error\": -0.8625035141561688,\n",
            "    \"mean_squared_error\": -0.7439123119317397,\n",
            "    \"mean_absolute_error\": -0.8165629131398799,\n",
            "    \"pearsonr\": 0.9080039381151497,\n",
            "    \"median_absolute_error\": -0.795024488144618\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_absolute_error': -0.8165629131398799,\n",
              " 'mean_squared_error': -0.7439123119317397,\n",
              " 'median_absolute_error': -0.795024488144618,\n",
              " 'pearsonr': 0.9080039381151497,\n",
              " 'r2': -0.5873478822793081,\n",
              " 'root_mean_squared_error': -0.8625035141561688}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shiba_model = LSTM()\n",
        "criterion = nn.MSELoss()\n",
        "optimiser = torch.optim.Adam(shiba_model.parameters(), lr=0.01)\n",
        "training_loop(n_epochs = 100,\n",
        "              model = shiba_model,\n",
        "              optimiser = optimiser,\n",
        "              loss_fn = criterion,\n",
        "              train_input = torch.tensor(shibaDataTrainInput.values.astype(np.float32)).unsqueeze(0),\n",
        "              train_target = torch.tensor(shibaDataTrainOutput.values.astype(np.float32)).unsqueeze(0),\n",
        "              test_input = torch.tensor(shibaDataTestInput.values.astype(np.float32)).unsqueeze(0),\n",
        "              test_target = torch.tensor(shibaDataTestOutput.values.astype(np.float32)).unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJh8eAd7it9x",
        "outputId": "a54a2aea-c8c1-4561-e9c6-5f63e2b2c484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST LOSS: tensor(0.8882)\n",
            "Step: 0, Loss: 0.7903334498405457\n",
            "TEST LOSS: tensor(0.7657)\n",
            "Step: 1, Loss: 0.540715754032135\n",
            "TEST LOSS: tensor(0.4824)\n",
            "Step: 2, Loss: 0.3224453628063202\n",
            "TEST LOSS: tensor(0.0346)\n",
            "Step: 3, Loss: 0.3807208240032196\n",
            "TEST LOSS: tensor(0.0888)\n",
            "Step: 4, Loss: 0.2202317714691162\n",
            "TEST LOSS: tensor(0.3347)\n",
            "Step: 5, Loss: 0.1580784022808075\n",
            "TEST LOSS: tensor(0.4644)\n",
            "Step: 6, Loss: 0.14665119349956512\n",
            "TEST LOSS: tensor(0.4171)\n",
            "Step: 7, Loss: 0.12487724423408508\n",
            "TEST LOSS: tensor(0.2214)\n",
            "Step: 8, Loss: 0.08039413392543793\n",
            "TEST LOSS: tensor(0.0438)\n",
            "Step: 9, Loss: 0.08412238210439682\n",
            "TEST LOSS: tensor(0.0647)\n",
            "Step: 10, Loss: 0.0939771756529808\n",
            "TEST LOSS: tensor(0.0403)\n",
            "Step: 11, Loss: 0.07047351449728012\n",
            "TEST LOSS: tensor(0.0574)\n",
            "Step: 12, Loss: 0.06976422667503357\n",
            "TEST LOSS: tensor(0.0780)\n",
            "Step: 13, Loss: 0.0759611651301384\n",
            "TEST LOSS: tensor(0.0751)\n",
            "Step: 14, Loss: 0.07395149767398834\n",
            "TEST LOSS: tensor(0.0561)\n",
            "Step: 15, Loss: 0.06609021127223969\n",
            "TEST LOSS: tensor(0.0390)\n",
            "Step: 16, Loss: 0.05932966619729996\n",
            "TEST LOSS: tensor(0.0361)\n",
            "Step: 17, Loss: 0.05704162269830704\n",
            "TEST LOSS: tensor(0.0433)\n",
            "Step: 18, Loss: 0.05713169649243355\n",
            "TEST LOSS: tensor(0.0463)\n",
            "Step: 19, Loss: 0.055445343255996704\n",
            "TEST LOSS: tensor(0.0395)\n",
            "Step: 20, Loss: 0.05106028914451599\n",
            "TEST LOSS: tensor(0.0310)\n",
            "Step: 21, Loss: 0.046858515590429306\n",
            "TEST LOSS: tensor(0.0293)\n",
            "Step: 22, Loss: 0.04527709260582924\n",
            "TEST LOSS: tensor(0.0339)\n",
            "Step: 23, Loss: 0.04567306488752365\n",
            "TEST LOSS: tensor(0.0383)\n",
            "Step: 24, Loss: 0.04569895938038826\n",
            "TEST LOSS: tensor(0.0379)\n",
            "Step: 25, Loss: 0.043442726135253906\n",
            "TEST LOSS: tensor(0.0332)\n",
            "Step: 26, Loss: 0.03963150456547737\n",
            "TEST LOSS: tensor(0.0283)\n",
            "Step: 27, Loss: 0.0371820442378521\n",
            "TEST LOSS: tensor(0.0261)\n",
            "Step: 28, Loss: 0.03691023588180542\n",
            "TEST LOSS: tensor(0.0257)\n",
            "Step: 29, Loss: 0.03679145500063896\n",
            "TEST LOSS: tensor(0.0281)\n",
            "Step: 30, Loss: 0.03500506281852722\n",
            "TEST LOSS: tensor(0.0368)\n",
            "Step: 31, Loss: 0.0325358510017395\n",
            "TEST LOSS: tensor(0.0515)\n",
            "Step: 32, Loss: 0.03122108429670334\n",
            "TEST LOSS: tensor(0.0654)\n",
            "Step: 33, Loss: 0.03088342770934105\n",
            "TEST LOSS: tensor(0.0716)\n",
            "Step: 34, Loss: 0.029991157352924347\n",
            "TEST LOSS: tensor(0.0684)\n",
            "Step: 35, Loss: 0.028168903663754463\n",
            "TEST LOSS: tensor(0.0596)\n",
            "Step: 36, Loss: 0.026692606508731842\n",
            "TEST LOSS: tensor(0.0517)\n",
            "Step: 37, Loss: 0.026426129043102264\n",
            "TEST LOSS: tensor(0.0500)\n",
            "Step: 38, Loss: 0.026039691641926765\n",
            "TEST LOSS: tensor(0.0558)\n",
            "Step: 39, Loss: 0.02457481622695923\n",
            "TEST LOSS: tensor(0.0682)\n",
            "Step: 40, Loss: 0.02336006611585617\n",
            "TEST LOSS: tensor(0.0825)\n",
            "Step: 41, Loss: 0.02298607863485813\n",
            "TEST LOSS: tensor(0.0931)\n",
            "Step: 42, Loss: 0.022449105978012085\n",
            "TEST LOSS: tensor(0.0973)\n",
            "Step: 43, Loss: 0.021176839247345924\n",
            "TEST LOSS: tensor(0.0973)\n",
            "Step: 44, Loss: 0.019894350320100784\n",
            "TEST LOSS: tensor(0.0988)\n",
            "Step: 45, Loss: 0.019322939217090607\n",
            "TEST LOSS: tensor(0.1081)\n",
            "Step: 46, Loss: 0.01887117698788643\n",
            "TEST LOSS: tensor(0.1276)\n",
            "Step: 47, Loss: 0.017989464104175568\n",
            "TEST LOSS: tensor(0.1526)\n",
            "Step: 48, Loss: 0.017309604212641716\n",
            "TEST LOSS: tensor(0.1737)\n",
            "Step: 49, Loss: 0.01692219264805317\n",
            "TEST LOSS: tensor(0.1842)\n",
            "Step: 50, Loss: 0.01632128469645977\n",
            "TEST LOSS: tensor(0.1859)\n",
            "Step: 51, Loss: 0.01569232903420925\n",
            "TEST LOSS: tensor(0.1887)\n",
            "Step: 52, Loss: 0.015376811847090721\n",
            "TEST LOSS: tensor(0.2037)\n",
            "Step: 53, Loss: 0.014919557608664036\n",
            "TEST LOSS: tensor(0.2330)\n",
            "Step: 54, Loss: 0.014403708279132843\n",
            "TEST LOSS: tensor(0.2654)\n",
            "Step: 55, Loss: 0.014254964888095856\n",
            "TEST LOSS: tensor(0.2880)\n",
            "Step: 56, Loss: 0.013845443725585938\n",
            "TEST LOSS: tensor(0.2998)\n",
            "Step: 57, Loss: 0.013372242450714111\n",
            "TEST LOSS: tensor(0.3156)\n",
            "Step: 58, Loss: 0.013279068283736706\n",
            "TEST LOSS: tensor(0.3488)\n",
            "Step: 59, Loss: 0.012754601426422596\n",
            "TEST LOSS: tensor(0.3889)\n",
            "Step: 60, Loss: 0.012531631626188755\n",
            "TEST LOSS: tensor(0.4138)\n",
            "Step: 61, Loss: 0.012307138182222843\n",
            "TEST LOSS: tensor(0.4252)\n",
            "Step: 62, Loss: 0.012040611356496811\n",
            "TEST LOSS: tensor(0.4478)\n",
            "Step: 63, Loss: 0.011859443970024586\n",
            "TEST LOSS: tensor(0.4821)\n",
            "Step: 64, Loss: 0.011715877801179886\n",
            "TEST LOSS: tensor(0.5010)\n",
            "Step: 65, Loss: 0.011578979901969433\n",
            "TEST LOSS: tensor(0.5055)\n",
            "Step: 66, Loss: 0.011456848122179508\n",
            "TEST LOSS: tensor(0.5222)\n",
            "Step: 67, Loss: 0.011334754526615143\n",
            "TEST LOSS: tensor(0.5446)\n",
            "Step: 68, Loss: 0.01131636742502451\n",
            "TEST LOSS: tensor(0.5502)\n",
            "Step: 69, Loss: 0.011109255254268646\n",
            "TEST LOSS: tensor(0.5508)\n",
            "Step: 70, Loss: 0.011074655689299107\n",
            "TEST LOSS: tensor(0.5633)\n",
            "Step: 71, Loss: 0.010884423740208149\n",
            "TEST LOSS: tensor(0.5713)\n",
            "Step: 72, Loss: 0.010811437852680683\n",
            "TEST LOSS: tensor(0.5652)\n",
            "Step: 73, Loss: 0.010608626529574394\n",
            "TEST LOSS: tensor(0.5623)\n",
            "Step: 74, Loss: 0.010509238578379154\n",
            "TEST LOSS: tensor(0.5679)\n",
            "Step: 75, Loss: 0.010351154953241348\n",
            "TEST LOSS: tensor(0.5666)\n",
            "Step: 76, Loss: 0.010222455486655235\n",
            "TEST LOSS: tensor(0.5580)\n",
            "Step: 77, Loss: 0.01010469812899828\n",
            "TEST LOSS: tensor(0.5556)\n",
            "Step: 78, Loss: 0.010005931369960308\n",
            "TEST LOSS: tensor(0.5580)\n",
            "Step: 79, Loss: 0.009914005175232887\n",
            "TEST LOSS: tensor(0.5535)\n",
            "Step: 80, Loss: 0.009816904552280903\n",
            "TEST LOSS: tensor(0.5446)\n",
            "Step: 81, Loss: 0.009753287769854069\n",
            "TEST LOSS: tensor(0.5417)\n",
            "Step: 82, Loss: 0.009668742306530476\n",
            "TEST LOSS: tensor(0.5419)\n",
            "Step: 83, Loss: 0.009600020945072174\n",
            "TEST LOSS: tensor(0.5363)\n",
            "Step: 84, Loss: 0.009516012854874134\n",
            "TEST LOSS: tensor(0.5281)\n",
            "Step: 85, Loss: 0.009450080804526806\n",
            "TEST LOSS: tensor(0.5257)\n",
            "Step: 86, Loss: 0.009359456598758698\n",
            "TEST LOSS: tensor(0.5263)\n",
            "Step: 87, Loss: 0.00929128285497427\n",
            "TEST LOSS: tensor(0.5233)\n",
            "Step: 88, Loss: 0.00920778512954712\n",
            "TEST LOSS: tensor(0.5202)\n",
            "Step: 89, Loss: 0.009134959429502487\n",
            "TEST LOSS: tensor(0.5232)\n",
            "Step: 90, Loss: 0.009044968523085117\n",
            "TEST LOSS: tensor(0.5283)\n",
            "Step: 91, Loss: 0.008974138647317886\n",
            "TEST LOSS: tensor(0.5296)\n",
            "Step: 92, Loss: 0.008884274400770664\n",
            "TEST LOSS: tensor(0.5309)\n",
            "Step: 93, Loss: 0.008809010498225689\n",
            "TEST LOSS: tensor(0.5367)\n",
            "Step: 94, Loss: 0.008723602630198002\n",
            "TEST LOSS: tensor(0.5417)\n",
            "Step: 95, Loss: 0.008651449345052242\n",
            "TEST LOSS: tensor(0.5427)\n",
            "Step: 96, Loss: 0.008569573052227497\n",
            "TEST LOSS: tensor(0.5451)\n",
            "Step: 97, Loss: 0.008497633971273899\n",
            "TEST LOSS: tensor(0.5496)\n",
            "Step: 98, Loss: 0.008423319086432457\n",
            "TEST LOSS: tensor(0.5503)\n",
            "Step: 99, Loss: 0.008344549685716629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dogeDataFull = DataCleaning.getDogeData()\n",
        "dogeDataTrainInput = dogeDataFull[0:456][allList] #excluding final month\n",
        "dogeDataTrainOutput = dogeDataFull[0:456][[\"standardized_price\"]]\n",
        "dogeDataTestInput = dogeDataFull[456:][allList] #final month\n",
        "dogeDataTestOutput = dogeDataFull[456:][[\"standardized_price\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xK8_NTtqEyy",
        "outputId": "97b520a1-4086-4b88-a6cf-3aa1f25c1964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (88) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/content/CryptoDataFull/DataCleaning.py:165: DtypeWarning: Columns (83,84) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:165: DtypeWarning: Columns (82,84,86,87,88) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:165: DtypeWarning: Columns (11,22,78,85,88,89,90,91,92,93,94) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:165: DtypeWarning: Columns (81,82,83,87,88,89) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['allPosts'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TabularDataset(dogeDataTrainInput)\n",
        "predictor_age3 = TabularPredictor(label=\"standardized_price\", path = \"newmodel3\", eval_metric=\"r2\").fit(train_data)\n",
        "TabularPredictor.load(\"newmodel3/\").evaluate(dogeDataTestInput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrdDcWYxd8K5",
        "outputId": "57d0bc20-5258-4119-b43a-8da48c471888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"newmodel3/\"\n",
            "AutoGluon Version:  0.4.1\n",
            "Python Version:     3.7.13\n",
            "Operating System:   Linux\n",
            "Train Data Rows:    456\n",
            "Train Data Columns: 7\n",
            "Label Column: standardized_price\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (4.047161173017568, -1.076357364208957, -0.06782, 1.1035)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11471.68 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 7 | ['standardized_txs', 'standardized_posts', 'standardized_tweets', 'standardized_trends', 'standardized_allTweets', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 7 | ['standardized_txs', 'standardized_posts', 'standardized_tweets', 'standardized_trends', 'standardized_allTweets', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t7 features in original data used to generate 7 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 364, Val Rows: 92\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.9007\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.9101\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.8883\t = Validation score   (r2)\n",
            "\t0.55s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.8627\t = Validation score   (r2)\n",
            "\t0.56s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t0.8965\t = Validation score   (r2)\n",
            "\t0.83s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.9032\t = Validation score   (r2)\n",
            "\t2.81s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t0.9114\t = Validation score   (r2)\n",
            "\t0.74s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.8472\t = Validation score   (r2)\n",
            "\t0.64s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t0.8492\t = Validation score   (r2)\n",
            "\t0.86s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.9306\t = Validation score   (r2)\n",
            "\t4.7s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9027\t = Validation score   (r2)\n",
            "\t0.64s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t0.9311\t = Validation score   (r2)\n",
            "\t0.49s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.26s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"newmodel3/\")\n",
            "Evaluation: r2 on test data: -0.20497076747394116\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"r2\": -0.20497076747394116,\n",
            "    \"root_mean_squared_error\": -0.3453367132938789,\n",
            "    \"mean_squared_error\": -0.11925744554861871,\n",
            "    \"mean_absolute_error\": -0.2844918584008436,\n",
            "    \"pearsonr\": 0.740415595407449,\n",
            "    \"median_absolute_error\": -0.25670112083861707\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_absolute_error': -0.2844918584008436,\n",
              " 'mean_squared_error': -0.11925744554861871,\n",
              " 'median_absolute_error': -0.25670112083861707,\n",
              " 'pearsonr': 0.740415595407449,\n",
              " 'r2': -0.20497076747394116,\n",
              " 'root_mean_squared_error': -0.3453367132938789}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doge_model = LSTM()\n",
        "criterion = nn.MSELoss()\n",
        "optimiser = torch.optim.Adam(doge_model.parameters(), lr=0.01)\n",
        "training_loop(n_epochs = 30,\n",
        "              model = doge_model,\n",
        "              optimiser = optimiser,\n",
        "              loss_fn = criterion,\n",
        "              train_input = torch.tensor(dogeDataTrainInput.values.astype(np.float32)).unsqueeze(0),\n",
        "              train_target = torch.tensor(dogeDataTrainOutput.values.astype(np.float32)).unsqueeze(0),\n",
        "              test_input = torch.tensor(dogeDataTestInput.values.astype(np.float32)).unsqueeze(0),\n",
        "              test_target = torch.tensor(dogeDataTestOutput.values.astype(np.float32)).unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6CIPSukqSa-",
        "outputId": "88db60b3-57f7-497c-ce80-b4665920dce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST LOSS: tensor(0.0178)\n",
            "Step: 0, Loss: 0.771108090877533\n",
            "TEST LOSS: tensor(0.0137)\n",
            "Step: 1, Loss: 0.5029100775718689\n",
            "TEST LOSS: tensor(0.0163)\n",
            "Step: 2, Loss: 0.29050835967063904\n",
            "TEST LOSS: tensor(0.0596)\n",
            "Step: 3, Loss: 0.36671000719070435\n",
            "TEST LOSS: tensor(0.0594)\n",
            "Step: 4, Loss: 0.3310298025608063\n",
            "TEST LOSS: tensor(0.0259)\n",
            "Step: 5, Loss: 0.23060977458953857\n",
            "TEST LOSS: tensor(0.0127)\n",
            "Step: 6, Loss: 0.20813709497451782\n",
            "TEST LOSS: tensor(0.0084)\n",
            "Step: 7, Loss: 0.21350792050361633\n",
            "TEST LOSS: tensor(0.0069)\n",
            "Step: 8, Loss: 0.20858928561210632\n",
            "TEST LOSS: tensor(0.0065)\n",
            "Step: 9, Loss: 0.18373385071754456\n",
            "TEST LOSS: tensor(0.0071)\n",
            "Step: 10, Loss: 0.14651402831077576\n",
            "TEST LOSS: tensor(0.0084)\n",
            "Step: 11, Loss: 0.12446318566799164\n",
            "TEST LOSS: tensor(0.0067)\n",
            "Step: 12, Loss: 0.12856870889663696\n",
            "TEST LOSS: tensor(0.0055)\n",
            "Step: 13, Loss: 0.10831144452095032\n",
            "TEST LOSS: tensor(0.0161)\n",
            "Step: 14, Loss: 0.08985543251037598\n",
            "TEST LOSS: tensor(0.0365)\n",
            "Step: 15, Loss: 0.08969386667013168\n",
            "TEST LOSS: tensor(0.0548)\n",
            "Step: 16, Loss: 0.09294384717941284\n",
            "TEST LOSS: tensor(0.0626)\n",
            "Step: 17, Loss: 0.08785098046064377\n",
            "TEST LOSS: tensor(0.0605)\n",
            "Step: 18, Loss: 0.0771455317735672\n",
            "TEST LOSS: tensor(0.0535)\n",
            "Step: 19, Loss: 0.06985752284526825\n",
            "TEST LOSS: tensor(0.0465)\n",
            "Step: 20, Loss: 0.06927336752414703\n",
            "TEST LOSS: tensor(0.0415)\n",
            "Step: 21, Loss: 0.06355363130569458\n",
            "TEST LOSS: tensor(0.0388)\n",
            "Step: 22, Loss: 0.05703389272093773\n",
            "TEST LOSS: tensor(0.0376)\n",
            "Step: 23, Loss: 0.058199964463710785\n",
            "TEST LOSS: tensor(0.0372)\n",
            "Step: 24, Loss: 0.06151106581091881\n",
            "TEST LOSS: tensor(0.0370)\n",
            "Step: 25, Loss: 0.0585467629134655\n",
            "TEST LOSS: tensor(0.0371)\n",
            "Step: 26, Loss: 0.051175713539123535\n",
            "TEST LOSS: tensor(0.0376)\n",
            "Step: 27, Loss: 0.0464041642844677\n",
            "TEST LOSS: tensor(0.0389)\n",
            "Step: 28, Loss: 0.046779971569776535\n",
            "TEST LOSS: tensor(0.0414)\n",
            "Step: 29, Loss: 0.04765256494283676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ethDataFull = DataCleaning.getEthereumData()\n",
        "ethDataTrainInput = ethDataFull[0:456][allList] #excluding final month\n",
        "ethDataTrainOutput = ethDataFull[0:456][[\"standardized_price\"]]\n",
        "ethDataTestInput = ethDataFull[456:][allList] #final month\n",
        "ethDataTestOutput = ethDataFull[456:][[\"standardized_price\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uMEnW_GtRzN",
        "outputId": "7dff8b65-8e18-45b3-8e86-8f20ac52f545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (83,86,87,91,92) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/content/CryptoDataFull/DataCleaning.py:286: DtypeWarning: Columns (83,84) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:286: DtypeWarning: Columns (82,84,86,87,88) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:286: DtypeWarning: Columns (11,22,78,85,88,89,90,91,92,93,94) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n",
            "/content/CryptoDataFull/DataCleaning.py:286: DtypeWarning: Columns (81,82,83,87,88,89) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  allData = pd.concat([dailyTwts, dailyPrice, dailyPosts, dailyTxs, modTrends, getAllData()], axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['allPosts'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TabularDataset(ethDataTrainInput)\n",
        "predictor_age4 = TabularPredictor(label=\"standardized_price\", path = \"newmodel4\", eval_metric=\"r2\").fit(train_data)\n",
        "TabularPredictor.load(\"newmodel4/\").evaluate(ethDataTestInput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mwdclgsfVhq",
        "outputId": "040dc448-69ce-4f9b-b1a3-7de5014ecde1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"newmodel4/\"\n",
            "AutoGluon Version:  0.4.1\n",
            "Python Version:     3.7.13\n",
            "Operating System:   Linux\n",
            "Train Data Rows:    456\n",
            "Train Data Columns: 7\n",
            "Label Column: standardized_price\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1.6832649724107107, -1.4051938863963773, -0.28354, 0.89559)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11445.21 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 7 | ['standardized_txs', 'standardized_posts', 'standardized_tweets', 'standardized_trends', 'standardized_allTweets', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 7 | ['standardized_txs', 'standardized_posts', 'standardized_tweets', 'standardized_trends', 'standardized_allTweets', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t7 features in original data used to generate 7 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.11s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 364, Val Rows: 92\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.9293\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.9378\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.9205\t = Validation score   (r2)\n",
            "\t0.55s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9419\t = Validation score   (r2)\n",
            "\t0.62s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t0.9408\t = Validation score   (r2)\n",
            "\t0.84s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.9423\t = Validation score   (r2)\n",
            "\t3.15s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t0.9301\t = Validation score   (r2)\n",
            "\t0.74s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.9107\t = Validation score   (r2)\n",
            "\t0.65s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t0.9343\t = Validation score   (r2)\n",
            "\t0.79s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.9579\t = Validation score   (r2)\n",
            "\t4.11s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9189\t = Validation score   (r2)\n",
            "\t0.66s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t0.9596\t = Validation score   (r2)\n",
            "\t0.5s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.02s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"newmodel4/\")\n",
            "Evaluation: r2 on test data: 0.35313278840963835\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"r2\": 0.35313278840963835,\n",
            "    \"root_mean_squared_error\": -0.4348734437893667,\n",
            "    \"mean_squared_error\": -0.18911491211322332,\n",
            "    \"mean_absolute_error\": -0.3587203231050805,\n",
            "    \"pearsonr\": 0.5954065462662949,\n",
            "    \"median_absolute_error\": -0.34140002166049394\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_absolute_error': -0.3587203231050805,\n",
              " 'mean_squared_error': -0.18911491211322332,\n",
              " 'median_absolute_error': -0.34140002166049394,\n",
              " 'pearsonr': 0.5954065462662949,\n",
              " 'r2': 0.35313278840963835,\n",
              " 'root_mean_squared_error': -0.4348734437893667}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eth_model = LSTM()\n",
        "criterion = nn.MSELoss()\n",
        "optimiser = torch.optim.Adam(eth_model.parameters(), lr=0.005)\n",
        "training_loop(n_epochs = 75,\n",
        "              model = eth_model,\n",
        "              optimiser = optimiser,\n",
        "              loss_fn = criterion,\n",
        "              train_input = torch.tensor(ethDataTrainInput.values.astype(np.float32)).unsqueeze(0),\n",
        "              train_target = torch.tensor(ethDataTrainOutput.values.astype(np.float32)).unsqueeze(0),\n",
        "              test_input = torch.tensor(ethDataTestInput.values.astype(np.float32)).unsqueeze(0),\n",
        "              test_target = torch.tensor(ethDataTestOutput.values.astype(np.float32)).unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVi9kCIptdiB",
        "outputId": "9805f8fe-a43c-4a67-dc53-400f359ae369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST LOSS: tensor(0.2962)\n",
            "Step: 0, Loss: 0.8886749148368835\n",
            "TEST LOSS: tensor(0.2688)\n",
            "Step: 1, Loss: 0.7133722901344299\n",
            "TEST LOSS: tensor(0.2357)\n",
            "Step: 2, Loss: 0.5363751649856567\n",
            "TEST LOSS: tensor(0.1922)\n",
            "Step: 3, Loss: 0.36924293637275696\n",
            "TEST LOSS: tensor(0.1326)\n",
            "Step: 4, Loss: 0.2939853072166443\n",
            "TEST LOSS: tensor(0.0621)\n",
            "Step: 5, Loss: 0.27756738662719727\n",
            "TEST LOSS: tensor(0.0206)\n",
            "Step: 6, Loss: 0.2140713483095169\n",
            "TEST LOSS: tensor(0.0583)\n",
            "Step: 7, Loss: 0.17181719839572906\n",
            "TEST LOSS: tensor(0.2109)\n",
            "Step: 8, Loss: 0.19390086829662323\n",
            "TEST LOSS: tensor(0.2944)\n",
            "Step: 9, Loss: 0.20551317930221558\n",
            "TEST LOSS: tensor(0.2641)\n",
            "Step: 10, Loss: 0.1853628009557724\n",
            "TEST LOSS: tensor(0.1959)\n",
            "Step: 11, Loss: 0.16148586571216583\n",
            "TEST LOSS: tensor(0.1387)\n",
            "Step: 12, Loss: 0.148185595870018\n",
            "TEST LOSS: tensor(0.1030)\n",
            "Step: 13, Loss: 0.1433713287115097\n",
            "TEST LOSS: tensor(0.0844)\n",
            "Step: 14, Loss: 0.14165018498897552\n",
            "TEST LOSS: tensor(0.0774)\n",
            "Step: 15, Loss: 0.14020952582359314\n",
            "TEST LOSS: tensor(0.0790)\n",
            "Step: 16, Loss: 0.13862892985343933\n",
            "TEST LOSS: tensor(0.0881)\n",
            "Step: 17, Loss: 0.13726483285427094\n",
            "TEST LOSS: tensor(0.1054)\n",
            "Step: 18, Loss: 0.13603317737579346\n",
            "TEST LOSS: tensor(0.1320)\n",
            "Step: 19, Loss: 0.13407035171985626\n",
            "TEST LOSS: tensor(0.1698)\n",
            "Step: 20, Loss: 0.1303226351737976\n",
            "TEST LOSS: tensor(0.2198)\n",
            "Step: 21, Loss: 0.12457206100225449\n",
            "TEST LOSS: tensor(0.2825)\n",
            "Step: 22, Loss: 0.11797656863927841\n",
            "TEST LOSS: tensor(0.3558)\n",
            "Step: 23, Loss: 0.11256852000951767\n",
            "TEST LOSS: tensor(0.4338)\n",
            "Step: 24, Loss: 0.1100194901227951\n",
            "TEST LOSS: tensor(0.5053)\n",
            "Step: 25, Loss: 0.11045253276824951\n",
            "TEST LOSS: tensor(0.5561)\n",
            "Step: 26, Loss: 0.11213292181491852\n",
            "TEST LOSS: tensor(0.5751)\n",
            "Step: 27, Loss: 0.11264963448047638\n",
            "TEST LOSS: tensor(0.5611)\n",
            "Step: 28, Loss: 0.11083683371543884\n",
            "TEST LOSS: tensor(0.5230)\n",
            "Step: 29, Loss: 0.10738281160593033\n",
            "TEST LOSS: tensor(0.4737)\n",
            "Step: 30, Loss: 0.10371982306241989\n",
            "TEST LOSS: tensor(0.4249)\n",
            "Step: 31, Loss: 0.10080454498529434\n",
            "TEST LOSS: tensor(0.3843)\n",
            "Step: 32, Loss: 0.09880596399307251\n",
            "TEST LOSS: tensor(0.3551)\n",
            "Step: 33, Loss: 0.09741514176130295\n",
            "TEST LOSS: tensor(0.3381)\n",
            "Step: 34, Loss: 0.09620871394872665\n",
            "TEST LOSS: tensor(0.3324)\n",
            "Step: 35, Loss: 0.09483365714550018\n",
            "TEST LOSS: tensor(0.3364)\n",
            "Step: 36, Loss: 0.09306317567825317\n",
            "TEST LOSS: tensor(0.3480)\n",
            "Step: 37, Loss: 0.09083937853574753\n",
            "TEST LOSS: tensor(0.3640)\n",
            "Step: 38, Loss: 0.08830799162387848\n",
            "TEST LOSS: tensor(0.3803)\n",
            "Step: 39, Loss: 0.08575035631656647\n",
            "TEST LOSS: tensor(0.3920)\n",
            "Step: 40, Loss: 0.08340704441070557\n",
            "TEST LOSS: tensor(0.3944)\n",
            "Step: 41, Loss: 0.08132229745388031\n",
            "TEST LOSS: tensor(0.3848)\n",
            "Step: 42, Loss: 0.07934494316577911\n",
            "TEST LOSS: tensor(0.3636)\n",
            "Step: 43, Loss: 0.07734188437461853\n",
            "TEST LOSS: tensor(0.3338)\n",
            "Step: 44, Loss: 0.07587715238332748\n",
            "TEST LOSS: tensor(0.2983)\n",
            "Step: 45, Loss: 0.07290644943714142\n",
            "TEST LOSS: tensor(0.2655)\n",
            "Step: 46, Loss: 0.07102569937705994\n",
            "TEST LOSS: tensor(0.2390)\n",
            "Step: 47, Loss: 0.06905074417591095\n",
            "TEST LOSS: tensor(0.2199)\n",
            "Step: 48, Loss: 0.06643567979335785\n",
            "TEST LOSS: tensor(0.2075)\n",
            "Step: 49, Loss: 0.06328301876783371\n",
            "TEST LOSS: tensor(0.1990)\n",
            "Step: 50, Loss: 0.0629047378897667\n",
            "TEST LOSS: tensor(0.1883)\n",
            "Step: 51, Loss: 0.0616132915019989\n",
            "TEST LOSS: tensor(0.1787)\n",
            "Step: 52, Loss: 0.061511240899562836\n",
            "TEST LOSS: tensor(0.1671)\n",
            "Step: 53, Loss: 0.05895256996154785\n",
            "TEST LOSS: tensor(0.1494)\n",
            "Step: 54, Loss: 0.05451376736164093\n",
            "TEST LOSS: tensor(0.1241)\n",
            "Step: 55, Loss: 0.04993081092834473\n",
            "TEST LOSS: tensor(0.0966)\n",
            "Step: 56, Loss: 0.04805423319339752\n",
            "TEST LOSS: tensor(0.0741)\n",
            "Step: 57, Loss: 0.04815369099378586\n",
            "TEST LOSS: tensor(0.0590)\n",
            "Step: 58, Loss: 0.04657028988003731\n",
            "TEST LOSS: tensor(0.0496)\n",
            "Step: 59, Loss: 0.04254498705267906\n",
            "TEST LOSS: tensor(0.0413)\n",
            "Step: 60, Loss: 0.042306121438741684\n",
            "TEST LOSS: tensor(0.0313)\n",
            "Step: 61, Loss: 0.03957148268818855\n",
            "TEST LOSS: tensor(0.0265)\n",
            "Step: 62, Loss: 0.040622301399707794\n",
            "TEST LOSS: tensor(0.0250)\n",
            "Step: 63, Loss: 0.037470828741788864\n",
            "TEST LOSS: tensor(0.0246)\n",
            "Step: 64, Loss: 0.035769470036029816\n",
            "TEST LOSS: tensor(0.0232)\n",
            "Step: 65, Loss: 0.03582781180739403\n",
            "TEST LOSS: tensor(0.0210)\n",
            "Step: 66, Loss: 0.03273797035217285\n",
            "TEST LOSS: tensor(0.0202)\n",
            "Step: 67, Loss: 0.031951505690813065\n",
            "TEST LOSS: tensor(0.0207)\n",
            "Step: 68, Loss: 0.032133180648088455\n",
            "TEST LOSS: tensor(0.0213)\n",
            "Step: 69, Loss: 0.030284933745861053\n",
            "TEST LOSS: tensor(0.0218)\n",
            "Step: 70, Loss: 0.02939591184258461\n",
            "TEST LOSS: tensor(0.0227)\n",
            "Step: 71, Loss: 0.029395489022135735\n",
            "TEST LOSS: tensor(0.0246)\n",
            "Step: 72, Loss: 0.027953850105404854\n",
            "TEST LOSS: tensor(0.0276)\n",
            "Step: 73, Loss: 0.027123846113681793\n",
            "TEST LOSS: tensor(0.0310)\n",
            "Step: 74, Loss: 0.027015801519155502\n"
          ]
        }
      ]
    }
  ]
}